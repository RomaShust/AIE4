{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomaShust/AIE4/blob/main/Session_5_Introduction_to_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ],
      "metadata": {
        "id": "gJXW_DgiSebM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ù Breakout Room #1"
      ],
      "metadata": {
        "id": "djQ3nRAgoF67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ],
      "metadata": {
        "id": "e7pQDUhUnIo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ],
      "metadata": {
        "id": "3_fLDElOVoop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaVwN269EttM",
        "outputId": "bc5b9e29-49ad-4485-8245-4fb62cbc4ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ],
      "metadata": {
        "id": "wujPjGJuoPwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "98bd9f62-1cf6-4987-8bef-15a720d7fc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "5930a63a-b7aa-461c-b8e0-78ec8c14a751"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ],
      "metadata": {
        "id": "sBRyQmEAVzua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ],
      "metadata": {
        "id": "2k6n_Dob2F46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(), # Create an instance of the tool class\n",
        "    ArxivQueryRun() # Create an instance of the tool class\n",
        "]"
      ],
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ],
      "metadata": {
        "id": "VI-C669ZYVI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ],
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.bind_tools(tool_belt)"
      ],
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### Answer #1:\n",
        "\n",
        "The model uses its language understanding capabilities to interpret the input from the user (e.g., a question or command). It looks for keywords, intents, and context that might suggest the use of a specific tool.\n",
        "\n",
        " Additionally, each tool in the tool_belt is typically associated with a specific type of task or input. The tools might have metadata, such as a name, description, or tags, which helps the model match the user's query to the appropriate tool."
      ],
      "metadata": {
        "id": "ERzuGo6W18Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ],
      "metadata": {
        "id": "_296Ub96Z_H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ],
      "metadata": {
        "id": "vWsMhfO9grLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ],
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ],
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ],
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ],
      "metadata": {
        "id": "b8CjRlbVmRpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ],
      "metadata": {
        "id": "uaXHpPeSnOWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ],
      "metadata": {
        "id": "BUsfGoSpoF9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ],
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ],
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ],
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ],
      "metadata": {
        "id": "yKCjWJCkrJb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ],
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ],
      "metadata": {
        "id": "KYqDpErlsCsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ],
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### Answer #2:\n",
        "\n",
        "In a graph-based approach in LangChain, there isn't an inherent limit to how many times you can cycle an agent through a graph. However, if we want to impose a limit on the number of cycles or iterations, we can do so by adding a counter or a similar control mechanism within the code that manages the execution of the graph."
      ],
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ],
      "metadata": {
        "id": "VEYcTShCsPaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "3acf2379-d23c-42b3-c4b4-9b5ae8d80ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Fx2PQyKeHON4PC8YNUvGmH3j', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-64b4fed6-fed4-4a3f-a860-0db48f909dc5-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_Fx2PQyKeHON4PC8YNUvGmH3j', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Adam Lowry was named captain of the Winnipeg Jets on Tuesday. ... Sep 20, 2023. Latest News. Inside look at Vegas Golden Knights Aug 30, 2024. Vegas Golden Knights fantasy projections for 2024-25 Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. Lowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ... Sep 13, 2023. Adam Lowry sat between his coach, Rick Bowness, and the GM who drafted him, Kevin Cheveldayoff, in front of an enormous, nine-panel screen at Winnipeg's newly renovated Scotia ...\", name='duckduckgo_search', tool_call_id='call_Fx2PQyKeHON4PC8YNUvGmH3j')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry, who was named captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 445, 'total_tokens': 471}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a2dff39-ebe9-4445-9466-7f6eb01014cd-0', usage_metadata={'input_tokens': 445, 'output_tokens': 26, 'total_tokens': 471})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ],
      "metadata": {
        "id": "DBHnUtLSscRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print('1')\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ad203428-ae3e-40a8-9ef2-a5c049dc7c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1ic7pDc0CVNnhg22OX2GhoRy', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_OtOdibsaDfoZdzdUi5c97Jfp', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_57kdiO5ZnpaQjfC2SUaR8oJ6', 'function': {'arguments': '{\"query\": \"Mike Lewis latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_77TVlWNobjr2MMUU3xM3a748', 'function': {'arguments': '{\"query\": \"Yunqing Dou latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_8G1c6r9TRj8Vh94mLJd4UXSh', 'function': {'arguments': '{\"query\": \"Armen Aghajanyan latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 173, 'total_tokens': 294}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2bfda692-71be-4c9f-af8b-4a1325142135-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_1ic7pDc0CVNnhg22OX2GhoRy', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_OtOdibsaDfoZdzdUi5c97Jfp', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Mike Lewis latest Tweet'}, 'id': 'call_57kdiO5ZnpaQjfC2SUaR8oJ6', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Yunqing Dou latest Tweet'}, 'id': 'call_77TVlWNobjr2MMUU3xM3a748', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Armen Aghajanyan latest Tweet'}, 'id': 'call_8G1c6r9TRj8Vh94mLJd4UXSh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 121, 'total_tokens': 294})]\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_1ic7pDc0CVNnhg22OX2GhoRy'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶ But Hixson\\'s ruling said Meta researcher Tim Dettmers, who was talking with other researchers from the nonprofit group EleutherAI on the instant messaging platform Discord, didn\\'t have the authority to waive attorney-client privilege. Democratic vice-presidential nominee and Minnesota Governor Tim Walz appeared on the popular social media account \"SubwayTakes\" Monday, marking the latest moment in the social media battle ... Part 1 of Dana Bash\\'s exclusive interview with 2024 Democratic presidential nominee Kamala Harris and her running mate Gov. Tim Walz, their first since the Democratic National Convention. Watch ...', name='duckduckgo_search', tool_call_id='call_OtOdibsaDfoZdzdUi5c97Jfp'), ToolMessage(content='Author Michael Lewis spent countless hours with FTX founder and alleged fraudster Sam Bankman-Fried. He details the cryptocurrency wunderkind\\'s rise and fall in his new book, \"Going Infinite.\" Latest Michael Lewis, the financial journalist who wrote a controversial book about the downfall of Sam Bankman-Fried, says the former crypto kingpin\\'s crime \"still makes no sense.\". Bankman-Fried was sentenced to 25 years in prison in March after being convicted of a colossal fraud in which he stole $8 billion of customers\\' deposits from his FTX cryptocurrency exchange. By Christopher Beam. Oct. 6, 2023. A hike is Michael Lewis\\'s interview format of choice. When he first met the FTX founder Sam Bankman-Fried in late 2021, he took the cargo-shorted chief ... By Andrew R. Chow. October 9, 2023 9:20 AM EDT. W hen the journalist Michael Lewis announced in May that he was writing a book on Sam Bankman-Fried after spending months with the FTX crypto mogul ... Author Michael Lewis met with FTX founder Sam Bankman-Fried more than 100 times. Lewis breaks down the crypto superstar\\'s rise and fall in his new book, \"Going Infinite.\" Latest', name='duckduckgo_search', tool_call_id='call_57kdiO5ZnpaQjfC2SUaR8oJ6'), ToolMessage(content='Yunqing\\'s friend, Law Yi Wei, reported the incident to authorities. \"He showed to police the picture of the victim in handcuffs as well as the police ID of M/Sgt. John Reggie Reyes with badge ... Latest Comments. wapz on Yang Zi and Liu Yu Ning in Talks for Female Empowerment Period C-drama Family Business; Jiao on Yang Zi and Liu Yu Ning in Talks for Female Empowerment Period C-drama Family Business; Mumma Bear on Kim Hye Yoon Wraps Up Her Fun Girl 2024 Summer Jetting Off to Taiwan for Fan Meeting 31st October 2023 - (Hong Kong) In a rare public appearance, Chinese actor Shawn Dou and late Macao casino tycoon Stanley Ho\\'s daughter, Laurinda Ho, attended a cultural event together, raising eyebrows and fueling speculation about the state of their five-month-old marriage. The couple, who tied the knot in a lavish ceremony in Bali earlier [‚Ä¶] Diving into the drama between Yanqing and Yunli on Honkai: Star Rail as Reddit users share their reactions. Jarvis the NPC. June 5, 2024. via HoYoverse. Honkai: Star Rail is experiencing some intense drama between the characters Yanqing and Yunli, sparking a frenzy of emotions among Reddit users. You bullied him, but he got you back. At the time, combining the tweet with the fact that DOOBU deleted all mentions of HYBE from his social media profiles as of April 2021, Korean news outlets speculated that things have gone awry between HYBE/BELIFT LAB and DOOBU. Fast forward to 2024, DOOBU\\'s latest tweet is receiving a tremendous amount of attention amid the Min Hee Jin vs. HYBE fallout.', name='duckduckgo_search', tool_call_id='call_77TVlWNobjr2MMUU3xM3a748'), ToolMessage(content='Armen Aghajanyan introduced Chameleon, FAIR\\'s latest work on multimodal models, training 7B and 34B models on 10T tokens of text and image ... Tweet from Armen Aghajanyan (@ArmenAgha): I\\'m excited to announce our latest paper, introducing a family of early-fusion token-in token-out (gpt4o‚Ä¶.), models capable of interleaved text and image ... Aram is a research scientist working at Fundamental AI Research (FAIR, Meta AI). His research focuses on various aspects of multimodal LLMs. Prior to joining Meta AI, he held various roles at universities, national laboratories, and industry. Aram received his first M.S. in pure mathematics from Yerevan State University in Armenia, the second M ... However, in May, Armen Aghajanyan, one of the people who worked on the project, wrote on X that their models \"were done training 5 months ago\" and claimed they\\'ve \"progressed significantly ... Armen Aghajanyan\\'s tweet indicates that Meta is making significant strides in AI development. By empowering the open-source community, Meta aims to democratize AI technology. This move could lead to the creation of powerful AI models that rival proprietary systems. ... Latest From Us: The Dark Side of AI Love: Why Experts Warn Against Digital ... Follow. The Fundamental AI Research (FAIR) team at Meta recently released Chameleon, a mixed-modal AI model that can understand and generate mixed text and image content. In experiments rated by ...', name='duckduckgo_search', tool_call_id='call_8G1c6r9TRj8Vh94mLJd4UXSh')]\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### QLoRA Paper on Arxiv\\n**Title:** QLoRA: Efficient Finetuning of Quantized LLMs  \\n**Authors:** Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer  \\n**Published:** 2023-05-23  \\n**Summary:** QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. The paper introduces several innovations to save memory without sacrificing performance, such as 4-bit NormalFloat (NF4), double quantization, and paged optimizers. The best model family, Guanaco, outperforms previous models on the Vicuna benchmark, reaching 99.3% of ChatGPT\\'s performance level with only 24 hours of finetuning on a single GPU.\\n\\n### Latest Tweets of Authors\\n\\n#### Tim Dettmers\\n- **Tweet:** Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model.\\n\\n#### Mike Lewis\\n- **Tweet:** Author Michael Lewis spent countless hours with FTX founder and alleged fraudster Sam Bankman-Fried. He details the cryptocurrency wunderkind\\'s rise and fall in his new book, \"Going Infinite.\"\\n\\n#### Yunqing Dou\\n- **Tweet:** Yunqing\\'s friend, Law Yi Wei, reported the incident to authorities. \"He showed to police the picture of the victim in handcuffs as well as the police ID of M/Sgt. John Reggie Reyes with badge ...\"\\n\\n#### Armen Aghajanyan\\n- **Tweet:** Armen Aghajanyan introduced Chameleon, FAIR\\'s latest work on multimodal models, training 7B and 34B models on 10T tokens of text and image.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 421, 'prompt_tokens': 2454, 'total_tokens': 2875}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-942f8811-6d24-4224-a430-93d079f28b91-0', usage_metadata={'input_tokens': 2454, 'output_tokens': 421, 'total_tokens': 2875})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Setting up tools to search in arXiv\n",
        "2. Search thorugh arXiv for authors of the manuscript\n",
        "3. Preparing tools to search in DuckDuckGo\n",
        "4. Pulling out tweeets\n",
        "5. Summarizing tweets in answer"
      ],
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LangSmith Evaluator"
      ],
      "metadata": {
        "id": "v7c8-Uyarh1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing for LangSmith"
      ],
      "metadata": {
        "id": "pV3XeFOT1Sar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ],
      "metadata": {
        "id": "wruQCuzewUuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ],
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "ddc3fec8-e606-412e-ae18-37fa9be5a12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model needs to produce responses based on a large and diverse set of information, such as in question-answering systems, chatbots, and other applications requiring detailed and accurate information retrieval.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "f9UkCIqkpyZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ],
      "metadata": {
        "id": "VfMXF2KAsQxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the purpose of the Adam optimizer in deep learning?\",\n",
        "    \"What is BERT used for in natural language processing?\",\n",
        "    \"What is the main advantage of using transformers over RNNs?\",\n",
        "    \"Who is credited with inventing the backpropagation algorithm?\",\n",
        "    \"What does dropout do in a neural network?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"momentum\", \"gradient\"]},\n",
        "    {\"must_mention\" : [\"pre-training\", \"contextual\"]},\n",
        "    {\"must_mention\" : [\"parallel\", \"computation\"]},\n",
        "    {\"must_mention\" : [\"Geoffrey\", \"Hinton\"]},\n",
        "    {\"must_mention\" : [\"regularization\", \"overfitting\"]},\n",
        "]"
      ],
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ],
      "metadata": {
        "id": "z7QVFuAmsh7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ],
      "metadata": {
        "id": "ciV73F9Q04w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ],
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ],
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "### Answer #4:\n",
        "The must_mention metric implemented in the code snippet is designed to evaluate whether a generated output contains specific required phrases. While the implementation is straightforward and functional, there are several gaps and areas for improvement:\n",
        "\n",
        "1. **Exact Matching vs. Partial Matching**\n",
        "\n",
        "The metric checks if each required phrase is exactly present in the prediction. This approach might fail in cases where the phrasing is slightly different (e.g., \"paged optimizer\" vs. \"optimizer with paging\") or if there is a slight variation in word order or synonyms.\n",
        "\n",
        "*Improvement*: Implement partial matching or synonym recognition using a natural language processing (NLP) library like spaCy or fuzzy matching with a library like fuzzywuzzy to allow for more flexible phrase matching.\n",
        "\n",
        "2. **Order Sensitivity**\n",
        "\n",
        "The metric does not consider the order in which phrases appear. For some tasks, the order of key phrases might be important (e.g., \"must mention A before B\").\n",
        "\n",
        "*Improvement*:  Add an option to consider the order of the phrases, allowing for more nuanced evaluation when needed.\n",
        "\n",
        "3. **Handling Variations in Spacing and Punctuation**\n",
        "\n",
        "The metric might not handle variations in spacing or punctuation well, leading to false negatives. If the phrases have different spacing or punctuation, they might not be recognized as a match.\n",
        "\n",
        "*Improvement*: Normalize spacing and punctuation in both the prediction and required phrases before comparison."
      ],
      "metadata": {
        "id": "PNtHORUh0jZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ],
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ],
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ],
      "metadata": {
        "id": "r1RJr349zhv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "82266280-b59b-4b33-c2fe-f82c3258ae66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 263e0348' at:\n",
            "https://smith.langchain.com/o/a0a0f90f-621f-4805-821f-aa4a12e57e1c/datasets/f2a4c75b-9671-4a8e-add0-34f9480069b0/compare?selectedSessions=0c7297e4-d5fb-4cb8-bbf5-ccd5925ca284\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 8a0b4377 at:\n",
            "https://smith.langchain.com/o/a0a0f90f-621f-4805-821f-aa4a12e57e1c/datasets/f2a4c75b-9671-4a8e-add0-34f9480069b0\n",
            "[------------------------------------------------->] 5/5\n",
            " Experiment Results:\n",
            "       feedback.must_mention error  execution_time                                run_id\n",
            "count                      5     0            5.00                                     5\n",
            "unique                     2     0             NaN                                     5\n",
            "top                     True   NaN             NaN  10ae59e0-a663-466c-84ca-e23d183243c8\n",
            "freq                       3   NaN             NaN                                     1\n",
            "mean                     NaN   NaN            4.30                                   NaN\n",
            "std                      NaN   NaN            1.24                                   NaN\n",
            "min                      NaN   NaN            2.86                                   NaN\n",
            "25%                      NaN   NaN            3.45                                   NaN\n",
            "50%                      NaN   NaN            4.53                                   NaN\n",
            "75%                      NaN   NaN            4.58                                   NaN\n",
            "max                      NaN   NaN            6.08                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 263e0348',\n",
              " 'results': {'e64d7b16-fd05-4326-b015-e22711f04001': {'input': {'question': 'What is the purpose of the Adam optimizer in deep learning?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('70dc68ec-c8c4-4678-b602-631b1c39a88e'), target_run_id=None)],\n",
              "   'execution_time': 4.534231,\n",
              "   'run_id': '10ae59e0-a663-466c-84ca-e23d183243c8',\n",
              "   'output': 'The Adam optimizer, which stands for Adaptive Moment Estimation, is a popular optimization algorithm used in deep learning. Its purpose is to improve the efficiency and effectiveness of the training process for neural networks. Here are some key points about the Adam optimizer:\\n\\n1. **Adaptive Learning Rates**: Adam computes adaptive learning rates for each parameter. This means that it adjusts the learning rate for each weight of the neural network individually, which can lead to faster convergence and better performance.\\n\\n2. **Combines Momentum and RMSProp**: Adam combines the advantages of two other popular optimization techniques: Momentum and RMSProp. Momentum helps accelerate gradients vectors in the right directions, thus leading to faster converging. RMSProp, on the other hand, helps in dealing with the problem of diminishing learning rates.\\n\\n3. **First and Second Moments**: Adam keeps track of both the first moment (mean) and the second moment (uncentered variance) of the gradients. This helps in stabilizing the learning process and makes the optimizer more robust to noisy gradients.\\n\\n4. **Bias Correction**: Adam includes bias-correction terms to account for the fact that the estimates of the first and second moments are biased towards zero, especially during the initial steps of training.\\n\\n5. **Efficiency**: Adam is computationally efficient and has low memory requirements, making it suitable for large datasets and high-dimensional parameter spaces.\\n\\n6. **Default Parameters**: The default parameters of Adam (learning rate, beta1, beta2, and epsilon) generally work well for most problems, which makes it a good starting point for many deep learning tasks.\\n\\nIn summary, the Adam optimizer is designed to provide a balance between fast convergence and robustness, making it a widely used choice for training deep learning models.',\n",
              "   'reference': {'must_mention': ['momentum', 'gradient']}},\n",
              "  '9c61c54b-8fb7-4e7c-b514-77a68ade374c': {'input': {'question': 'What is BERT used for in natural language processing?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c405c87e-8dbc-49cc-a4b9-ea4325806c76'), target_run_id=None)],\n",
              "   'execution_time': 4.581269,\n",
              "   'run_id': '78a23137-b071-433d-a78c-3f4d54d72d05',\n",
              "   'output': \"BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art model used in natural language processing (NLP) for a variety of tasks. Here are some of its primary uses:\\n\\n1. **Text Classification**: BERT can be fine-tuned for text classification tasks such as sentiment analysis, spam detection, and topic categorization.\\n\\n2. **Named Entity Recognition (NER)**: BERT can identify and classify entities in text into predefined categories such as names of people, organizations, locations, etc.\\n\\n3. **Question Answering**: BERT can be used to build question-answering systems where it can understand a question and provide an accurate answer from a given context.\\n\\n4. **Text Summarization**: BERT can help in generating summaries of long documents by understanding the context and extracting the most relevant information.\\n\\n5. **Machine Translation**: BERT can be used to improve the quality of machine translation systems by providing better contextual understanding.\\n\\n6. **Part-of-Speech Tagging**: BERT can be used to tag parts of speech in a sentence, which is useful for syntactic parsing and other linguistic analyses.\\n\\n7. **Coreference Resolution**: BERT can help in determining which words in a sentence refer to the same entity, which is crucial for understanding the context in longer texts.\\n\\n8. **Text Generation**: BERT can be used in generating coherent and contextually relevant text, which is useful in applications like chatbots and content creation.\\n\\n9. **Semantic Similarity**: BERT can be used to measure the similarity between two pieces of text, which is useful in tasks like duplicate question detection and paraphrase identification.\\n\\nBERT's bidirectional nature allows it to understand the context of a word based on all of its surrounding words, making it highly effective for these NLP tasks.\",\n",
              "   'reference': {'must_mention': ['pre-training', 'contextual']}},\n",
              "  'daaf70df-0503-4002-9aac-597d9e46654d': {'input': {'question': 'What is the main advantage of using transformers over RNNs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('816b1fd2-5246-4c76-88f8-883ef4d440a7'), target_run_id=None)],\n",
              "   'execution_time': 3.446875,\n",
              "   'run_id': 'ecbd5799-1955-4c66-b8ef-562859a2ca85',\n",
              "   'output': 'The main advantage of using transformers over Recurrent Neural Networks (RNNs) lies in their ability to handle long-range dependencies and parallelize computations. Here are some key points:\\n\\n1. **Parallelization**: Transformers allow for parallel processing of input data, whereas RNNs process data sequentially. This makes transformers significantly faster, especially for long sequences.\\n\\n2. **Long-Range Dependencies**: Transformers can capture long-range dependencies more effectively than RNNs. RNNs, including LSTMs and GRUs, struggle with long-term dependencies due to the vanishing gradient problem. Transformers use self-attention mechanisms that can directly connect distant positions in the sequence.\\n\\n3. **Scalability**: Transformers scale better with larger datasets and more complex models. They have been shown to perform exceptionally well on large-scale tasks, such as language modeling and machine translation.\\n\\n4. **Flexibility**: The self-attention mechanism in transformers is more flexible and can be adapted to various tasks beyond sequence modeling, such as image processing and reinforcement learning.\\n\\n5. **State Management**: RNNs maintain a hidden state that is passed from one time step to the next, which can be cumbersome to manage. Transformers, on the other hand, do not rely on a hidden state, simplifying the architecture.\\n\\nOverall, these advantages make transformers a more powerful and efficient choice for many natural language processing tasks and other applications involving sequential data.',\n",
              "   'reference': {'must_mention': ['parallel', 'computation']}},\n",
              "  '35bafac0-330d-4b72-bde7-61d1e4e940ab': {'input': {'question': 'Who is credited with inventing the backpropagation algorithm?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('42607ee8-37b6-4a1a-90fe-27fd91dc1f5b'), target_run_id=None)],\n",
              "   'execution_time': 2.857483,\n",
              "   'run_id': 'c2f66e52-09a1-418c-a988-d6b1519b3bee',\n",
              "   'output': \"The backpropagation algorithm, which is fundamental to training artificial neural networks, is often credited to multiple researchers. The key contributions are:\\n\\n1. **Paul Werbos**: He is widely recognized for his 1974 Ph.D. thesis, where he described the backpropagation algorithm. His work laid the groundwork for the algorithm's development and application in neural networks.\\n\\n2. **David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams**: In 1986, these researchers published a highly influential paper that popularized the backpropagation algorithm in the context of training multi-layer neural networks. Their work demonstrated the practical utility of backpropagation and significantly advanced the field of neural networks.\\n\\nWhile Paul Werbos is credited with the initial development, the 1986 paper by Rumelhart, Hinton, and Williams played a crucial role in bringing the algorithm to the forefront of machine learning research.\",\n",
              "   'reference': {'must_mention': ['Geoffrey', 'Hinton']}},\n",
              "  '1a4bcbb2-7516-4525-b684-f42ca92cf4b3': {'input': {'question': 'What does dropout do in a neural network?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9caf6c6f-5760-4ef2-8b69-cf2ce5be4ccb'), target_run_id=None)],\n",
              "   'execution_time': 6.081563,\n",
              "   'run_id': 'd9793736-e344-48af-99b7-d3c03e344255',\n",
              "   'output': 'Dropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly \"dropping out\" (i.e., setting to zero) a fraction of the neurons during the training process. This means that during each training iteration, a different subset of neurons is ignored, which forces the network to learn more robust features that are not reliant on specific neurons.\\n\\nHere are some key points about dropout:\\n\\n1. **Random Neuron Deactivation**: During each training step, a certain percentage of neurons are randomly selected and temporarily removed from the network. This percentage is defined by a hyperparameter called the dropout rate (e.g., 0.5 means 50% of the neurons are dropped out).\\n\\n2. **Preventing Overfitting**: By deactivating neurons randomly, dropout prevents the network from becoming too dependent on any particular neuron. This helps in reducing overfitting, where the model performs well on training data but poorly on unseen data.\\n\\n3. **Improving Generalization**: Dropout encourages the network to learn more general features that are useful across different subsets of neurons, thereby improving the model\\'s ability to generalize to new data.\\n\\n4. **Training and Inference**: During training, dropout is applied to the neurons. However, during inference (i.e., when making predictions on new data), dropout is not applied. Instead, the weights of the neurons are scaled down by the dropout rate to account for the fact that more neurons are active during inference.\\n\\n5. **Implementation**: Dropout is typically implemented as a layer in neural network libraries such as TensorFlow and PyTorch. It can be easily added to any part of the network.\\n\\nIn summary, dropout is a simple yet powerful technique to improve the performance and robustness of neural networks by reducing overfitting and encouraging the learning of more general features.',\n",
              "   'reference': {'must_mention': ['regularization', 'overfitting']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ],
      "metadata": {
        "id": "jhTNe4kWrplB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ],
      "metadata": {
        "id": "w1wKRddbIY_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ],
      "metadata": {
        "id": "npTYJ8ayR5B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ],
      "metadata": {
        "id": "sD7EV0HqSQcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ],
      "metadata": {
        "id": "oajBwLkFVi1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of a new state graph based on the AgentState, which represents the current state of the agent during its operation.\n",
        "  \n",
        "Adding two nodes: \"agent\" and \"Action\""
      ],
      "metadata": {
        "id": "M6rN7feNVn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ],
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sets the \"agent\" node as the entry point for the graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "XZ22o2mWVrfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below defines a function to determine the next step in an agent's process based on the state of its messages. If the last message includes a tool call, it transitions to the \"action\" node. Otherwise, it assesses the helpfulness of the agent's final response using a model, deciding whether to end the process or continue based on the model's evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ],
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ],
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the **tool_call_or_helpful** function is being integrated into the graph_with_helpfulness_check as a conditional decision-maker."
      ],
      "metadata": {
        "id": "6BhnBW2YVsJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding a direct edge from the \"action\" node back to the \"agent\" node"
      ],
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling into a usable agent by converting the defined state graph into an executable form."
      ],
      "metadata": {
        "id": "rSI8AOaEVvT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ],
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing querries asynchronously. The **'async for'** loop iterates through the agent's responses in real-time as it processes the input, printing the current node being executed and the associated messages for each step in the process, allowing you to see the agent's decisions and outputs as they are generated."
      ],
      "metadata": {
        "id": "F67FGCMRVwGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "ff957580-7e8f-48e0-e27e-23b852db17b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_3n3b3ZSzmBArF81nXeT4VAsb', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_ExIHRcwQHLEx9Ng1E589mRiL', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_NRTR8h96QkvosUXJPocGB6Ym', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a7fe1c8d-26ad-4422-9084-cf7029d7120b-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_3n3b3ZSzmBArF81nXeT4VAsb', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_ExIHRcwQHLEx9Ng1E589mRiL', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_NRTR8h96QkvosUXJPocGB6Ym', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Let\\'s jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA\\'s approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model\\'s learning capability is preserved with significantly ... This is where Low-Rank Adaptation (LoRA) technique appeared to be a game-changer enabling fine-tuning very big LLMs to specific task on limited resources and datasets. LoRA introduces a seemingly-simple yet powerful and cost-effective way to fine-tune LLMs and adapt them to a specific task by integrating low-rank matrices into the model\\'s ... \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. The paper \"LoRA: Low-Rank Adaptation of Large Language Models, ... Exploring the modern wave of machine learning: cutting edge fine tuning. Nov 7, 2023. 10. Matthew Gunton. in.', name='duckduckgo_search', tool_call_id='call_3n3b3ZSzmBArF81nXeT4VAsb'), ToolMessage(content='‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. MatFormer: Nested Transformer for Elastic Inference Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶ Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public.', name='duckduckgo_search', tool_call_id='call_ExIHRcwQHLEx9Ng1E589mRiL'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. An Attention Mechanism in deep learning simulates the human cognitive process of selective focus. It enables neural networks to assign varying degrees of importance to different parts of input ... The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ... The introduction of the Transformer model was a significant leap forward for the concept of attention in deep learning. Vaswani et al. described this model in the seminal paper titled \"Attention is All You Need\" in 2017. ... Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize ...', name='duckduckgo_search', tool_call_id='call_NRTR8h96QkvosUXJPocGB6Ym')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a technique used to fine-tune large language models (LLMs) efficiently. Traditional fine-tuning involves adjusting the pre-trained weights of a model using new training data, which can be computationally expensive and resource-intensive. LoRA addresses this by decomposing the weight updates into a product of lower-rank matrices. This approach maintains the model\\'s learning capability while significantly reducing the computational resources required. It allows for the adaptation of large pre-trained models to new tasks with limited resources and datasets.\\n\\n### Tim Dettmers\\nTim Dettmers is an AI researcher known for his work in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He specializes in quantization and the use of deep learning hardware accelerators. Dettmers has contributed to significant advancements in the field, such as the QLoRA approach, which drastically reduces the GPU memory required to fine-tune large models. He is associated with the Allen School and has joined Ai2 as an AI researcher.\\n\\n### Attention in Machine Learning\\nAttention mechanisms are a fundamental component in modern deep learning models, particularly in natural language processing (NLP) and computer vision. Inspired by the human cognitive process of selective focus, attention mechanisms allow neural networks to assign varying degrees of importance to different parts of the input data. This capability enhances the performance of models in tasks such as machine translation, image captioning, and speech recognition.\\n\\nThe concept of attention gained prominence with the introduction of the Transformer model, described in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017. Attention mechanisms enable models to handle longer sequences more effectively by focusing on relevant parts of the input, thus improving the overall performance and efficiency of deep learning models.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 1077, 'total_tokens': 1452}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c3a635e-be44-4eb1-8a59-db19007f2bf1-0', usage_metadata={'input_tokens': 1077, 'output_tokens': 375, 'total_tokens': 1452})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ],
      "metadata": {
        "id": "yVmZPs6lnpsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ],
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "5751d53a-6c7b-4290-882c-ccc75af5d1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models (LLMs) like GPT-3. It involves the design and optimization of prompts (input queries or instructions) to elicit the most accurate, relevant, and useful responses from AI models. The goal is to frame questions or tasks in a way that maximizes the performance of the AI in generating desired outputs.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Prompt Design**: Crafting the input text to guide the AI model towards producing a specific type of response.\n",
            "2. **Optimization**: Iteratively refining prompts based on the quality of the responses received.\n",
            "3. **Contextualization**: Providing sufficient context within the prompt to ensure the AI understands the task or question.\n",
            "4. **Evaluation**: Assessing the effectiveness of different prompts in achieving the desired outcomes.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became particularly prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to harness their full potential. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to systematically explore and document techniques for optimizing AI interactions.\n",
            "\n",
            "To get more precise information on the timeline and development of prompt engineering, I can look up recent articles or papers on the topic. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by researchers at Facebook AI in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published on arXiv in 2020. The technique leverages both retrieval and generation to handle knowledge-intensive tasks more effectively than either approach alone.\n",
            "\n",
            "Would you like more detailed information or the specific paper on RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Aspects of Fine-Tuning:\n",
            "1. **Pre-trained Model**: Start with a model that has been trained on a large dataset.\n",
            "2. **New Dataset**: Use a smaller, task-specific dataset to further train the model.\n",
            "3. **Adaptation**: The model adjusts its parameters to better perform the new task while retaining the general knowledge from the initial training.\n",
            "\n",
            "### Benefits:\n",
            "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
            "- **Performance**: Often results in better performance on specific tasks due to the pre-existing knowledge in the model.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and transfer learning techniques. It gained significant attention with the success of models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) in the late 2010s. These models demonstrated that fine-tuning could achieve state-of-the-art results in various natural language processing tasks.\n",
            "\n",
            "To pinpoint the exact timeline and key milestones, I can look up more detailed information. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, are a type of artificial intelligence that leverages large language models to perform tasks, interact with environments, and make decisions. These agents use the capabilities of LLMs to understand and generate human-like text, enabling them to perform a wide range of functions, from code generation to natural language processing.\n",
            "\n",
            "### Key Points:\n",
            "1. **Concept**: LLM-based agents interact with their environment by perceiving data, processing it, and taking actions based on the information. They can be part of multi-agent systems where each agent has a specific role.\n",
            "2. **Applications**: They are used in various domains, including software engineering, code generation, vulnerability detection, and more.\n",
            "3. **Research and Development**: The development of LLM-based agents has been a significant focus in both academia and industry, leading to numerous studies and advancements.\n",
            "\n",
            "### Timeline:\n",
            "- **Initial Development**: The concept of LLM-based agents began gaining traction with the advent of large language models like GPT-3, which was released by OpenAI in June 2020.\n",
            "- **Rise in Popularity**: Since then, the technology has seen rapid development and increased interest, particularly in the last few years, as researchers and developers explore its potential applications and capabilities.\n",
            "\n",
            "In summary, LLM-based agents broke onto the scene around 2020 with the release of advanced large language models and have since become a promising area of research and application in AI.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}